{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 1 - Introdução a Machine Leaning",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoAreias/Curso-ML/blob/master/Aula_1_Introdu%C3%A7%C3%A3o_a_Machine_Leaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9aZUnKotnX5",
        "colab_type": "text"
      },
      "source": [
        "# O que é Machine Learning\n",
        "\n",
        "Machine learning é a area da inteligência artificial que envolve desenvolver programas que realizam tarefas específicas sem programar explicitamente a tarefa que deve ser realizada. Isso é feito através de modelos estatísticos e o pensamento indutivo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_p2V8nEVld0",
        "colab_type": "text"
      },
      "source": [
        "### Dedução vs Indução\n",
        "\n",
        "Ao implementar um modelo de inteligência existem duas linhas de pensamento, o método dedutivo e o método indutivo. A dedução consiste em começar com premiças verdadeiras e raciocinar sobre as premiças para chegar a uma conclusão lógica. A indução consiste em analizar as premiças como evidências da sua verdade e chegar a uma verdade provável. A dedução garante resultados verdadeiros porém depende que as premoças sejam verdadeiras, enquanto a indução resulta em algo provavelmente verdadeiro e só necessita do conhecimento das evidências apresentadas. Um exemplo do contraste entre dedução e indução pode ser visto na tabela 1.\n",
        "\n",
        "<table>\n",
        "  <th>Pensamento dedutivo</th>\n",
        "  <th>Pensamento indutivo</th>\n",
        "  <tr>\n",
        "    <td><ul>\n",
        "      <li>Sócrates é um homem.</li>\n",
        "      <li>Todo homem é mortal.</li>\n",
        "      <li>Logo sócrates é mortal.</li>\n",
        "      </ul></td>\n",
        "    <td><ul>\n",
        "      <li>Todas as formas de vida que nós <br> conhecemos precisam de água.</li>\n",
        "      <li>Logo uma forma de vida extraterrestre <br> precisará de água.</li>\n",
        "      </ul></td>\n",
        "  </tr>\n",
        "  <tr><td colspan=2> Tabela 1:  Pensamentos dedutivo e indutivo</td></tr>\n",
        "</table>\n",
        "\n",
        "Até o meio dos anos 2000 o método dedutivo era muito popular e usado na maioria das apicações de inteligência artificial. Com o advento do big data e de avanços em GPUs, os métodos indutivos ganharam espaço e hoje apresentam melhores resultados do que os métodos dedutivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwHyx5b2VeCW",
        "colab_type": "text"
      },
      "source": [
        "### Métodos dedutivos\n",
        "\n",
        "Os métodos dedutivos tem um pensamento top-down em relação a sua lógica. Primeiro considera-se regras que valem para todo o domínio (premiças) assim através do reducionismo chega-se as conclusões finais. Métodos dedutivos exigem que as regras sejam pré-programadas no sistema assim não são muito utilizados na área de machine learning, porém podem ser usados após o aprendizado de máquina para criar raciocínios elaborados baseando-se nos dados, aprendidos, um exemplo disso é no algorítimo Monte-Carlo tree search, algorítimo usado no programa Alpha Go em 2016 que derrotou o campeão mundial de Go.\n",
        "\n",
        "É importante mencionar que métodos dedutivos tiveram um grande impacto no desenvolvimento da inteligência artificial como um todo, principalmente na computação simbólica, lógica de primeira ordem e em sistemas especialistas. Para aprender mais sobre o assunto a seguinte literatura é recomendada:\n",
        "\n",
        " - [Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp](https://github.com/norvig/paip-lisp)\n",
        " - [Prolog Programming for Artificial Intelligence](https://www.amazon.com/Programming-Artificial-Intelligence-International-Computer/dp/0321417461)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQLlI8pecfgj",
        "colab_type": "text"
      },
      "source": [
        "### Métodos indutivos\n",
        "\n",
        "Os métodos indutivos tem um pensamento bottom-up, nele primeiro se observa os dados e assim se cria as regras para descrever os dados. No pensamento indutivo, as regras tem uma probabilidade de serem verdades porém, nunca é possível saber com $100\\%$ de certeza de a regra está realmente correta. \n",
        "\n",
        "O sol nacerá amanhã? Imagine a resposta para essa pergunta, o sol naceu hoje, e ontem  nos ultimos 4 bilhões de anos, então nós temos uma confiança muito grande de que amanhã não será diferente. Nada impede porém de ele ser absorvido por um buraco negro ou até mesmo desaparecer por algum mecanismo que nós ainda nem conhecemos, mas nós temos certeza de que o sol nacerá amanhã.\n",
        "\n",
        "Este tipo de pensamento exemplifica o pensamento indutivo. A certeza que teremos quanto a um evento pode ser calculada usando estatística e aprimorada conforme novos eventos vão acontecendo. Todos os modelos de aprêndizado de máquinas trabalham com o pensamento indutivo e usam isso para construir modelos estatísticos da realidade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SXel40EtypJ",
        "colab_type": "text"
      },
      "source": [
        "# Classificação e regressão\n",
        "\n",
        "Problemas de machine learning são, na maioria dos casos problemas de classificação ou problemas de regressão. Problemas de classificação são aqueles onde dadas $N$ classes e um dado $x$, deve-se dizer em qual classe  $x$ pertence. Problemas de regreção são aqueles onde, dados $N$ pontos deve-se encontrar a curva que passa, ou aproxima, estes pontos. Nos dois casos, o problema consiste em minimizar alguma medida de erro responsável por avaliar a performace do seu programa. Podemos experimentar com estes dois problemas no [Tensorflow playground](https://playground.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvqbIDu2zlkG",
        "colab_type": "text"
      },
      "source": [
        "### Problemas de classificação\n",
        "\n",
        "Problemas de  classificação são aqueles onde, dado $N$ classes e um valor $x$, deve-se determinar a qual das classes $x$ percence. Considere o exemplo.\n",
        "\n",
        "EXEMPLO 1.1:\n",
        "\n",
        "Deve-se classificar se um animal é um cachorro, um gato ou um peixe. Podemos fazer isso percorrendo a seguinte árvore:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/8pV6Cep.png\"/>\n",
        "</center>\n",
        "\n",
        "Podemos classificar um novo animal simplesmente percorrendo esta árvore.\n",
        "\n",
        "#### Métricas para a classificação\n",
        "\n",
        "Como dito anteriormente,  problemas de machine learning funcionam minimizando uma métrica e para entender esta métrica primeiro é necessário entender a tabela 2. Suponha que queira predizer se um dado $x$ pertence ou não a uma classe, então existem 4 possíbilidades de predição onde 2 representam acertos e 2 representam erros.\n",
        "\n",
        "<table>\n",
        "  <th></th>\n",
        "  <th>Pertence</th>\n",
        "  <th>Não pertence</th>\n",
        "  <tr>\n",
        "    <th>Classificado como<br>pertence</th>\n",
        "    <td>Verdadeiro positivo<br>$T_p$</td>\n",
        "    <td>Falso positivo<br>$F_p$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Classificado como<br>não pertence</th>\n",
        "    <td>Falso negativo<br>$F_n$</td>\n",
        "    <td>Verdadeiro negativo<br>$T_n$</td>\n",
        "  </tr>\n",
        "  <tr><td colspan=2> Tabela 2:  tipos de erro</td></tr>\n",
        "</table>\n",
        "\n",
        "Falso positivos e falso negativos representam erros do tipo 1 e tipo 2 respectivamente, assim, é importante internalizar a imagem a seguir:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/dOpsrDR.jpg\" width=\"400px\"/>\n",
        "</center>\n",
        "\n",
        "As métricas podem ser encontradas abaixo:\n",
        "\n",
        "- Recall\n",
        "    \n",
        "    $$\\frac{T_p}{T_p + F_n}$$\n",
        "    \n",
        "    Recall representa a fração de elementos pertencente a classe que foram corretamente classificados. Este teste é usado quando classificar erroneamente um positivo como negativo pode ser muito danoso. Considere por exemplo um teste de cancer, enquanto um teste positivo pode ser averiguado com a biópsia, se o passiente testar negativo ele difícilmente tentará averiguar o resultado, assim um falso negativo é certamente letal. Este valor vai de 0 a 1 e quanto maior melhor. \n",
        "\n",
        "- Precisão\n",
        "    \n",
        "    $$\\frac{T_p}{T_p + F_p}$$\n",
        "    \n",
        "    A precisão representa a fração de elementos corretamente classificados como pertencentes a classe. Geralmente é utilizada quando classificar um negativo erroneamente como positivo pode ser danoso. Considere o exemplo, um teste de gravidez de farmácia apontou positivo, este teste tem uma baixa precisão, assim, ates de ir comprando as roupas de bebê e fazendo o pré natal, é recomendado fazer o teste  no hospital que tem uma precisão alta para confirmar o resultado. Este valor vai de 0 a 1 e quanto maior melhor. \n",
        "    \n",
        "- Acurácia\n",
        "    \n",
        "    $$\\frac{T_p + T_n}{T_p + T_n + F_p + F_n}$$\n",
        "    \n",
        "    Acurácia representa a fração de pontos que foram classificados corretamente, é a métrica mais presente nos trabalhs mais antigos de classificação. Otimizar a acurácia significa minimizar os erros sistemáticos. Este valor vai de 0 a 1 e quanto maior melhor. \n",
        "    \n",
        "- Entropia cruzada\n",
        "\n",
        "    $$\\sum -p_i\\cdot log_2(q_i)$$\n",
        "\n",
        "    Esta formula vem da teoria da informação e representa o quanto dois valores estão relacionados. $P$ representa a probabilidade de uma váriavel ser da classe $X$ e $Q$ representa a probabilidade de uma variável ser da classe $Y$, no contexto do aprendizado de máquina, $p_i$ representa o valor verdadeiro da classe da variável, e $q_i$ representa a probabilidade da variável ser da classe segundo o modelo. Esta função penaliza fortemente os erros tendendo rapidamente ao infinito para erros muito grandes e minimiza a probabilidade de erro. Este valor vai de 0 a $\\infty$ e quanto menor melhor.\n",
        "    \n",
        "- Cohen's Kappa e Weighted Kappa\n",
        "\n",
        "    $$\\kappa = \\frac{p_o - p_e}{1 - p_e} = 1 - \\frac{1 - p_o}{1 - p_e}$$\n",
        "\n",
        "    Cohen's kappa é uma medida controversa pois é de difícil interpretabilidade mas que elimina alguns dos problemas de se utilizar a acurácia em dados desbalanceados. No Cohen's kappa, $p_o$ representa a probabilidade de se observar a classe corretamente e é identico a acurácia, $p_e$ representa a probabilidade de classificar o dado corretamente de forma aleatória. Considere por exemplo os dados na tabela 3:\n",
        "    \n",
        "    EXEMPLO 1.2:\n",
        "    <table>\n",
        "  <th></th>\n",
        "  <th>A</th>\n",
        "  <th>B</th>\n",
        "  <tr>\n",
        "    <th>Classificado<br>como A</th>\n",
        "    <td>$20$</td>\n",
        "    <td>$5$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Classificado<br>como B</th>\n",
        "    <td>$10$</td>\n",
        "    <td>$15$</td>\n",
        "  </tr>\n",
        "  <tr><td colspan=2> Tabela 3:  Exemplo de Cohen's kappa</td></tr>\n",
        "</table>\n",
        "    Podemos calcular $p_o$ e $p_e$ da seginte forma:\n",
        "    \n",
        "    $$p_o = \\frac{20 + 15}{50} = 0.7$$\n",
        "    \n",
        "    $$p_A = \\frac{20 + 10}{50}\\cdot\\frac{20 + 5}{50} = 0.3$$\n",
        "    \n",
        "    $$p_B = \\frac{15 + 5}{50}\\cdot\\frac{15 + 10}{50} = 0.2$$\n",
        "    \n",
        "    $$p_e = p_A + p_B = 0.5$$\n",
        "    \n",
        "    Assim achamos um valor de $\\kappa$ de $0.4$\n",
        "    \n",
        "    Valores de $\\kappa$ vão de $\\left(-\\infty, 1\\right]$ sendo:\n",
        "     - $-\\infty \\lt \\kappa \\lt 0$ A classificação é pior do que classificar de forma aleatória\n",
        "     - $\\kappa = 0$ A classificação é tão boa quanto classificar de forma aleatória\n",
        "     - $\\kappa > 0$ A classificação é melhor do que aleatória\n",
        "     \n",
        "  Weighted kappa é similar ao Cohen's kappa porém um peso é associado ao erro de cada classe, este tipo de métrica é bom para casos onde alguns tipos de erro são piores que outros, por exemplo, é mais perigoso o seu algorítimo confundir um tigre com um gato do que um gato com um cachorro. Assim, ao otimizar o weighted kappa, as métricas mais importantes são dadas prioridades.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEwoUiXGv7FD",
        "colab_type": "text"
      },
      "source": [
        "### Regra de Bayes\n",
        "\n",
        "Ao falar de machine learning e de classificação é importante falar de regra de Bayes, uma das regras mais importantes para este método indutivo. A regra de Bayes parte de que existe um conhecimento prévio sobre o assunto, é realizado um experimento onde evidências são coletadas e assim esse conhecimento prévio é atualizado segundo os dados do experimento. Ela é escrita na seguinte forma:\n",
        "\n",
        "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
        "\n",
        "EXEMPLO 1.3:\n",
        "\n",
        "Um teste de uma doença degenerativa específica tem precisão de $99.9\\%$, esta doença acontece em apenas $0.001\\%$ da população, se você testou positivo para esta doença, qual é a probabilidade de que você realmente tenha a doença?\n",
        "\n",
        "$$P(doenca|+) = \\frac{P(+|doença) \\cdot P(doença)}{P(+)}$$\n",
        "\n",
        "$$P(doenca|+) = \\frac{0.999 \\cdot 0.00001}{0.999\\cdot 0.00001 + 0.001*0.99999} \\approx 0.989 \\% $$\n",
        "\n",
        "ou seja, mesmo testando positivo a sua chance de ter a doença é menor do que $1\\%$\n",
        "\n",
        "\n",
        "A regra de Bayes nos dá uma forma de interpretar medidas como a precisão, onde em casos em que as classes estão desbalanceadas podem ser enganosas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBNrhkrN6obx",
        "colab_type": "text"
      },
      "source": [
        "### Problemas de regressão\n",
        "\n",
        "Problemas de regressão são aqueles onde, dado um conjunto de pontos e assumindo um modelo é encontrado o medelo que melhor se aproxima dos dados apresentados. \n",
        "\n",
        "EXEMPLO 1.4:\n",
        "\n",
        "Considere o conjunto de pontos $(-3, 5), (1, 4)$ e $(2, 2)$ assumindo que o modelo é uma parábola, podemos encontrar a parábola que passa por estes pontos da seguinte forma:\n",
        "\n",
        "Seja os pontos descritos pela parábola $y = ax^2 + bx + c$, vamos definir uma matriz $A$, um vetor $\\vec{x}$ e um  vetor $\\vec{b}$ tal que:\n",
        "\n",
        "$$A = \\left[\\begin{matrix}\n",
        "x_1^2 & x_1 & 1 \\\\\n",
        "x_2^2 & x_2 & 1 \\\\\n",
        "x_3^2 & x_3 & 1 \\\\\n",
        "\\end{matrix}\\right], \\vec{x} = \\left[\\begin{matrix}\n",
        "a \\\\\n",
        "b \\\\\n",
        "c \\\\\n",
        "\\end{matrix}\\right], \\vec{b} = \\left[\\begin{matrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "y_3 \\\\\n",
        "\\end{matrix}\\right]$$\n",
        "\n",
        "\n",
        "\n",
        "assim precisamos apenas resolver o sistema de equações:\n",
        "\n",
        "$$A\\vec{x} = \\vec{b}$$\n",
        "\n",
        "Multiplicando os dois lados por $A^{-1}$ temos que:\n",
        "\n",
        "$$\\vec{x} = A^{-1}\\vec{b}$$\n",
        "\n",
        "\n",
        "assim, para os pontos no exemplo:\n",
        "\n",
        "$$\\left[\\begin{matrix}\n",
        "a \\\\\n",
        "b \\\\\n",
        "c \\\\\n",
        "\\end{matrix}\\right] = \n",
        "\\left[\\begin{matrix}\n",
        "9 & -3 & 1 \\\\\n",
        "1 & 1 & 1 \\\\\n",
        "4 & 2 & 1 \\\\\n",
        "\\end{matrix}\\right]^{-1}\n",
        "\\left[\\begin{matrix}\n",
        "5 \\\\\n",
        "4 \\\\\n",
        "2 \\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "Assim, executando o código abaixo vemos que\n",
        "\n",
        "$$\\left[\\begin{matrix}\n",
        "a \\\\\n",
        "b \\\\\n",
        "c \\\\\n",
        "\\end{matrix}\\right] = \n",
        "\\left[\\begin{matrix}\n",
        "-0.35 \\\\\n",
        "-0.95 \\\\\n",
        "5.3 \\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x67vBwYvsy2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.linalg import inv\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([\n",
        "    [9, -3, 1],\n",
        "    [1,  1, 1],\n",
        "    [4,  2, 1]\n",
        "])\n",
        "\n",
        "b = np.array([5, 4, 2])\n",
        "x = np.matmul(inv(A), b)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JIFP-tDfEEf",
        "colab_type": "text"
      },
      "source": [
        "Podemos plotar os dados para conferir a resposta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fQ0NEgxfC3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.polynomial.polynomial import Polynomial\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Pontos do exercício\n",
        "p = np.array([\n",
        "    [-3, 5],\n",
        "    [1, 4],\n",
        "    [2, 2]\n",
        "])\n",
        "\n",
        "# Polinomio encontrado\n",
        "y = Polynomial([5.3, -0.95, -0.35])\n",
        "x = np.arange(-5, 3, 0.1)\n",
        "\n",
        "# Plotando os dados\n",
        "plt.figure(figsize=(16, 8), facecolor='w')\n",
        "plt.plot(x, y(x))\n",
        "plt.plot(p[:, 0], p[:, 1], 'ro')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jWkB-T9jt3n",
        "colab_type": "text"
      },
      "source": [
        "A regressão permite que nós extrapolemos dados mesmo sem fazer medições. Alguns exemplos de uso da regressão são na predição do comportamento de sistemas dinâmicos, predição do valor de ações,  interpolação ao ampliar imagens e modelos geradores de músicas e de imagens.\n",
        "\n",
        "Como dito anteriormente, o uso destes algorítimos exige uma métrica para ser otimizada, no exemplo anterior o medelo se encaixa perfeitamente nos dados mas na maioria dos cenários reais este não é o caso. Dados podem conter ruídos e, apesar de o modelo aproximar bem os dados, é praticamente inexistente os casos de machine learning onde se sabe o modelo de antemão (até porque se o modelo ideal já fosse conhecido não haveria porque passar pelo algorítimo de aprendizagem). Assim, algumas destas métricas são mostradas a seguir:\n",
        "\n",
        "#### Métricas para a regressão:\n",
        "\n",
        "Seja $y_i$ o i-ésimo valor real dos seus pontos e $\\hat{y_i}$ o i-ésimo valor predizido pela sua regressão, definimos as métricas do erro na predição dos $N$ pontos como:\n",
        "\n",
        "\n",
        "- Mean absolute error (MAE):\n",
        "\n",
        "    $${1 \\over n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|$$\n",
        "    \n",
        "    Esta métrica possui a grande vantagem de ser medida na mesma unidade da predição, porém, o ganho obtido por diminuir o erro de 30 unidades para 20 unidades é o mesmo de reduzir o erro de 20 unidades para 10 unidades, assim, erros muito grandes não são muito penalizados. Por este motivo, esta métrica vem sido abandonada.\n",
        "\n",
        "\n",
        "- Mean squared error (MSE):\n",
        "\n",
        "    $${1 \\over n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
        "    \n",
        "  Esta mértica consiste em calcular a média do quadrado dos erros e tem o efeito de penalizar fortemente erros grandes  porém penalizar pouco erros pequenos. Ela é largamente utilizada nos problemas de regressão.\n",
        "\n",
        "- Root mean squared error (RMSE):\n",
        "\n",
        "  $$\\sqrt{{1 \\over n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$$\n",
        "    \n",
        "    Esta métrica apresenta o melhor dos dois mundos, por realizar a soma dos quadrados, ela penaliza mais erros grandes do que erros pequenos e ao retirar a raiz quadrada, a unidade da métrica é a mesma dos valores preditos, facilitando a interpretabilidade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1saEjWiuFv9",
        "colab_type": "text"
      },
      "source": [
        "# Problemas de otimização\n",
        "\n",
        "A aprendizagem de máquina então é nada mais nada menos do que otimizar modelos de caixa preta para otimizar as métricas mensionadas. Para isso, existem diversos algorítimos de otimização, dos quais nós vamos ver 3 dos mais importantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgcV9kxT6sZL",
        "colab_type": "text"
      },
      "source": [
        "### Descida em gradiente\n",
        "\n",
        "A descida em gradiente talvez seja o algorítimo mais importante para o aprendizado de máquina atualmente, quando o modelo é diferenciável com certeza é o que consegue os melhores resultados.\n",
        "\n",
        "Considere que você quer minimizar uma função $f(\\vec{x})$ o vetor gradiente $\\nabla f(\\vec{x})$ aponta para a o sentido onde o deslocamento neste sentido indica o maior incremento, assim, para minimizar $f$ da-se um passo na direção oposta a do gradiente, assim, a descida em gradiente consiste em iterar a função a seguir até convergir para um mínimo aceitável, onde $\\alpha$ é a taxa de aprendizado:\n",
        "\n",
        "$$\\vec{x_{n+1}} = \\vec{x_n} - \\alpha\\nabla f(\\vec{x_n})$$\n",
        "\n",
        "EXEMPLO 1.5\n",
        "\n",
        "Vamos achar o mínimo da função $f(x) = x^4 + 3x$, temos que $f'(x) = 4x^3 + 3$ e vamos adotar $x_0 = 0$ e $\\alpha = 0.001$. Para ter uma intuição podemos fazer as 3 primeiras iterações na mão, assim:\n",
        "\n",
        "$x_1 = x_0 - 0.001 \\cdot f'(x_0) = 0 - 0.003 = -0.003$\n",
        "\n",
        "$x_2 = x_1 - 0.001 \\cdot f'(x_1) = -0.003  - 0.003 = -0.006$\n",
        "\n",
        "$x_3 = x_2 - 0.001 \\cdot f'(x_2) = -0.006  - 0.003 = -0.009$\n",
        "\n",
        "Isso seria entediante sem o uso do computador então vamos programar este algorítimo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3xMsJo767Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def f(x):\n",
        "  return x**4 + 3*x\n",
        "\n",
        "\n",
        "def df(x):\n",
        "  return 4*x**3 + 3\n",
        "\n",
        "\n",
        "# Descida em gradiente\n",
        "x = 0\n",
        "alpha = 0.001\n",
        "\n",
        "for epoch in range(1000):\n",
        "  x = x - alpha*df(x)\n",
        "  \n",
        "# Plotando os resultados\n",
        "plt.figure(figsize=(16, 8), facecolor='w')\n",
        "# Plota a função\n",
        "t = np.linspace(-1.45, 0, 100)\n",
        "y = np.vectorize(f)(t)\n",
        "plt.plot(t, y)\n",
        "plt.plot(x, f(x), 'ro')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVurUeik-Usi",
        "colab_type": "text"
      },
      "source": [
        "Apesar de ser um excelente método de otimização, este método possui alguns problemas, detre eles alguns mais importantes são:\n",
        "\n",
        "\n",
        "- Mínimos locais\n",
        "\n",
        "    A descida em gradiente não consegue distinguir entre o mínimo local e o mínimo global, assim, uma vez que $\\vec{x}$ alcança um mínimo local ele tende a estabilizar lá e não encontra o mínimo global.\n",
        "    \n",
        "- Instabilidade vs tempo\n",
        "\n",
        "    A escolha de $\\alpha$ é algo que deve ser feito com cuidado, um valor muito pequeno de $\\alpha$ faz com que o sistema demore de mais para convergir enquanto um valor muito grande faz com que $\\vec{x}$ oscile e não encontre o mínimo. Atualmente não existe um método para a escolha de $\\alpha$ porém existem métodos adaptativos que mudam o valor de $\\alpha$ conforme a execução do algorítimo.\n",
        "    \n",
        "- Desaparecimento vs explosão do gradiente\n",
        "\n",
        "    Suponha que a função que você deseja otimizar é representada por $f(\\vec{x}) = g(h(i(\\vec{x})))$ o gradiente dessa função é dado por $\\nabla g(h(i(\\vec{x}))) \\cdot \\nabla h(i(\\vec{x})) \\cdot \\nabla i(\\vec{x})$, e multiplicação em cadeia faz com que o gradiente cresça ou diminua exponencialmente, isso é ainda mais sério em casos como redes neurais onde cada camada é representada como uma função das camadas anteriores e onde, as vezes, centenas de camadas são utilizadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byS3q9yi7NpT",
        "colab_type": "text"
      },
      "source": [
        "### Algorítimos evolutivos\n",
        "\n",
        "Algorítimos evolutivos são baseados na evolução de Charles Darwin. Soluções do problema são codificadas como um DNA, assim, diverças soluções aleatórias são geradas, mantendo a analogia a evolução, estas soluções são chamadas de indivíduos e o conjunto de soluções é chamado de população. Os indivíduos que melhor se encaixam na resposta se reproduzem, ou seja, novas soluções são gerados a partir destas soluções, enquanto os piores morrem naquela geração. \n",
        "\n",
        "Existem 3 operações básicas em algorítimos evolutivos:\n",
        "\n",
        " - Seleção:\n",
        " \n",
        "     Nesta etapa os indivíduos são selecionados a partir de uma função fitness que mede a performace do indivíduo como resposta. Os indivíduos com melhores valores de fitness são selecionados enquanto os outros são descartados.\n",
        "     \n",
        " - Cross-over:\n",
        " \n",
        "    Nesta etapa um novo indivíduo é gerado através de partes do DNA de 2 indivíduos diferentes, este passo acontece durante a reprodução dos indivíduos.\n",
        "    \n",
        " - Mutação:\n",
        " \n",
        "    Nesta etapa, o DNA do indivíduo pode mudar de forma aleatória.\n",
        "    \n",
        "    \n",
        " Estes processos são repetidos até que a resposta venha a convergir para um valor de fitness aceitável. Algorítimos genéticos são uma boa opção para quando o problema não é diferenciável ou quando se tem uma distribuição multimodal, ou seja, existem multiplos mínimos e máximos locais.\n",
        "O estudo de algorítimos evolutivos é extenso e foge do escopo deste curso, para aprofundar no assunto, o livro [Evolutionary Optimization Algorithms](https://www.amazon.com/Evolutionary-Optimization-Algorithms-Dan-Simon/dp/0470937416) é recomendado.\n",
        "\n",
        "\n",
        "EXEMPLO 1.6\n",
        "\n",
        "Vamos achar uma solução boa para o problema do caixeiro viajante no grafo a seguir:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui6yI81Ru1LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Criando o grafo\n",
        "grafo = nx.Graph()\n",
        "# Adicionando vétices\n",
        "n_vertices = 15\n",
        "vertices = np.arange(1, n_vertices+1)\n",
        "pos = zip(np.cos(2*np.pi*vertices/n_vertices), np.sin(2*np.pi*vertices/n_vertices))\n",
        "for vertice, p in zip(vertices, pos):\n",
        "  grafo.add_node(vertice, pos=p)\n",
        "\n",
        "# Adicionando arestas\n",
        "for i in range(1, n_vertices+1):\n",
        "  for j in range(i+1, n_vertices+1):\n",
        "    grafo.add_edge(i, j, weight=np.random.randint(1, 10))\n",
        "\n",
        "# Plotando o grafo\n",
        "plt.figure(figsize=(8, 8), facecolor='w')\n",
        "pos=nx.get_node_attributes(grafo,'pos')\n",
        "nx.draw(grafo, pos, with_labels=True)\n",
        "labels = nx.get_edge_attributes(grafo,'weight')\n",
        "nx.draw_networkx_edge_labels(grafo,pos,edge_labels=labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoGiN9vMpTYT",
        "colab_type": "text"
      },
      "source": [
        "Vamos definir o DNA como sendo uma permutação dos nós. A função fitness é a distância total percorrida. Para a mutação, dois nós adjacentes na permutação serão trocados e por último, para o cross-over, parte do primeiro indivíduo será copiada mantendo a posição e partes do segundo vão sendo adicionadas sem repetição. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKTTloaUwCNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "AVISO!!\n",
        "A célula de código anterior deve ser executada\n",
        "antes de executar esta célula\n",
        "\"\"\"\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "class AlgoritimoGenetico:\n",
        "  \n",
        "  def __init__(self, grafo):\n",
        "    self.GRAPH_MATRIX = nx.to_numpy_matrix(grafo)\n",
        "    self.GRAPH = grafo\n",
        "    # Hyperparametros\n",
        "    self.MUTATION_RATE = 0.05  # Probabilidade de haver mutação \n",
        "    self.POPULATION_SIZE = 300 # Tamanho da população\n",
        "    self.SELECTION_N = 50      # Quantidade de indivíduos que irão se reproduzir\n",
        "    self.ELITISM_SIZE = 10     # Quantos elementos são usados para elitismo\n",
        "    self.HISTORY = []          # Guarda os valores do melhor fitness\n",
        "    # Inicializa população\n",
        "    self.POPULATION = [\n",
        "        {'dna': np.random.permutation(grafo.nodes), 'fitness':float('inf')} \n",
        "        for _ in range(self.POPULATION_SIZE)\n",
        "    ]\n",
        "    \n",
        "    \n",
        "  def _fitness(self, individuo):\n",
        "    \"\"\" \n",
        "    Recebe um indivíduo e calcula a sua função fitness\n",
        "    \"\"\"\n",
        "    fitness = 0\n",
        "    for idx in range(individuo['dna'].shape[0]):\n",
        "      i = int(individuo['dna'][idx-1])\n",
        "      j = int(individuo['dna'][idx])\n",
        "      fitness += self.GRAPH_MATRIX[i-1, j-1]\n",
        "      \n",
        "    return fitness\n",
        "  \n",
        "  def _mutation(self, individuo):\n",
        "    \"\"\"\n",
        "    Recebe um indivíduo e retorna o dna dele mutado\n",
        "    \"\"\"\n",
        "    dna = individuo['dna']\n",
        "    for idx in range(dna.shape[0]):\n",
        "      if np.random.rand() < self.MUTATION_RATE:\n",
        "        dna[idx-1], dna[idx] = dna[idx], dna[idx-1]\n",
        "    \n",
        "    return dna\n",
        "  \n",
        "  def _crossover(self, individuo1, individuo2):\n",
        "    \"\"\"\n",
        "    Recebe 2 individuos e retona 1 novo individuo resultado do crossover deles\n",
        "    \"\"\"\n",
        "    dna1 = individuo1['dna']\n",
        "    dna2 = individuo2['dna']\n",
        "    size = dna1.shape[0]\n",
        "    # Pontos onde o DNA vai ser copiado\n",
        "    start, end = sorted(np.random.randint(size, size=2))\n",
        "    # Inicializa os filhos\n",
        "    child = np.zeros(size)\n",
        "    # Copia parte do DNA\n",
        "    child[start:end] = dna1[start:end]\n",
        "    # Copia o resto do DNA\n",
        "    i, j = 0, 0\n",
        "    while j < size:\n",
        "      # Pula valores já setados\n",
        "      if child[j] > 0:\n",
        "        j += 1\n",
        "        continue\n",
        "        \n",
        "      # Adiciona caso o valor não esteja la\n",
        "      if dna2[i] not in child:\n",
        "        child[j] = dna2[i]\n",
        "      \n",
        "      i += 1\n",
        "      \n",
        "    return {'dna': child, 'fitness': float('inf')}\n",
        "  \n",
        "  def fit(self, n_generations):\n",
        "    \"\"\"\n",
        "    Executa o algorítimo genético\n",
        "    \"\"\"\n",
        "    for _ in tqdm_notebook(range(n_generations)):\n",
        "      \n",
        "      # Calcula a função fitness\n",
        "      for individual in self.POPULATION:\n",
        "        individual['fitness'] = self._fitness(individual)\n",
        "      \n",
        "      # Realiza a seleção\n",
        "      self.POPULATION = sorted(self.POPULATION, key=lambda x: x['fitness'])[:self.SELECTION_N]\n",
        "      self.HISTORY.append(self.POPULATION[0]['fitness'])\n",
        "      \n",
        "      # Cria a nova população\n",
        "      new_population = self.POPULATION[:self.ELITISM_SIZE]\n",
        "      while len(new_population) < self.POPULATION_SIZE:\n",
        "        indv1, indv2 = np.random.choice(self.POPULATION, size=2, replace=False)\n",
        "        new_individual = self._crossover(indv1, indv2)\n",
        "        new_individual['dna'] = self._mutation(new_individual)\n",
        "        new_population.append(new_individual)\n",
        "        \n",
        "      self.POPULATION = new_population\n",
        "      \n",
        "    return self.POPULATION[0]\n",
        "  \n",
        "  def plot(self):\n",
        "    # Melhor caminho\n",
        "    indv = self.POPULATION[0]['dna']\n",
        "    caminho = [(indv[i-1], indv[i]) for i in range(indv.shape[0])]\n",
        "    plt.figure(figsize=(16, 8), facecolor='w')\n",
        "    \n",
        "    # Plota o grafo\n",
        "    plt.subplot(1, 2, 1)\n",
        "    pos=nx.get_node_attributes(grafo,'pos')\n",
        "    nx.draw(grafo, pos, with_labels=True)\n",
        "    labels = nx.get_edge_attributes(grafo,'weight')\n",
        "    nx.draw_networkx_edge_labels(grafo,pos,edge_labels=labels)\n",
        "    nx.draw_networkx_edges(grafo,pos=pos,edgelist=caminho, edge_color='r')\n",
        "    \n",
        "    # Plota a função fitness\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(self.HISTORY)\n",
        "    plt.xlabel('Gerações')\n",
        "    plt.ylabel('Melhor fitness')\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "ga = AlgoritimoGenetico(grafo)\n",
        "ga.fit(100)\n",
        "ga.plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOsDpVOj5gzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ga.POPULATION[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA7RCsFhNo4k",
        "colab_type": "text"
      },
      "source": [
        "### Pseudo inversa\n",
        "\n",
        "Se o seu modelo pode ser escrito na forma:\n",
        "\n",
        "$$y = a_1x_1 + a_2x_2 + \\cdots + a_nx_n $$\n",
        "\n",
        "Podemos escrever os nossos dados na forma de 3 matrizes:\n",
        "\n",
        "$$A = \\left[\\begin{matrix}\n",
        "x_{11} & x_{12} & \\cdots &  x_{1n} \\\\\n",
        "x_{21} & x_{22} & \\cdots &  x_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{m1} & x_{m2} & \\cdots &  x_{mn} \\\\\n",
        "\\end{matrix}\\right], \\vec{x} = \\left[\\begin{matrix}\n",
        "a_1 \\\\\n",
        "a_2 \\\\\n",
        "\\vdots \\\\\n",
        "a_n\n",
        "\\end{matrix}\\right], \\vec{b} = \\left[\\begin{matrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_m\n",
        "\\end{matrix}\\right]$$\n",
        "\n",
        "Assim queremos achar o vetor $\\vec{x}$ que melhor se encaixa nos dados, quando os dados não são ruidosos e $A$ é quadrada, é possível fazer a mesma operação do exemplo 1.4, mas este nunca é o caso em situações práticas. A prova deste método foge do escopo deste curso, mas pode ser encontrada [aqui](https://www.youtube.com/watch?v=EnNH3SxyZEI).\n",
        "\n",
        "Considere que:\n",
        "\n",
        "$$A\\vec{x} = \\vec{b}$$\n",
        "\n",
        "Como $A$ não é quadrada, não podemos inverter a matriz, mas $A^TA$ é quadrada, então fazemos:\n",
        "\n",
        "$$A^TA\\vec{x} = A^T\\vec{b}$$\n",
        "\n",
        "Multiplicando os dois lados por $(A^TA)^{-1}$ temos que:\n",
        "\n",
        "$$\\vec{x} = (A^TA)^{-1}A^T\\vec{b}$$\n",
        "\n",
        "A matriz $(A^TA)^{-1}A^T$ é a pseudo-inversa de $A$, resolver essa equação oferece os mínimo erro quadratico (mse) em uma regressão. Este método é bom para quando se tem poucos dados, porém ao aumentar o número de dados o tempo de processamento cresce de forma cúbica com a quantidade de dados, assim, não é muito ideal para trabalhar com muitos dados e pode tornar completamente inviável para cenários de Big Data.\n",
        "\n",
        "\n",
        "EXEMPLO 1.7\n",
        "\n",
        "Vamos fazer a regressão da função a seguir considerando que nossos dados são ruidosos e querendo minimizar o erro quadrático:\n",
        "\n",
        "$$y = a*x^3 + b*e^{-x} + c$$\n",
        "\n",
        "Para o exemplo vamos tentar achar a função:\n",
        "\n",
        "$$y = 3*x^3 + 5*e^{-x} + 2$$\n",
        "\n",
        "Esta primeira etapa do código simula a coleta de dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk4laf20G7ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.linspace(-2.6, 1.6, 10001)\n",
        "# 0.5 é descontado para o ruído ter média 0\n",
        "y = 3*x**3 + 5*np.exp(-x) + 2 + (np.random.rand(x.shape[0]) - 0.5)\n",
        "dados = np.vstack([x, y])\n",
        "del x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNuAjFpavDZ",
        "colab_type": "text"
      },
      "source": [
        "Podemos averiguar a seguir o ruído dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD0EVIi9auJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(16, 8), facecolor='w')\n",
        "plt.plot(dados[0], dados[1], 'ro', markersize=0.5)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZByu3_7bha3",
        "colab_type": "text"
      },
      "source": [
        "Vamos agora estimar os parâmetros pelo calculo da pseudo-inversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELcQW2Tfbgtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matrizes com os parâmetros\n",
        "A = np.array([dados[0]**3, np.exp(-dados[0]), np.ones(dados.shape[1])]).T\n",
        "b = dados[1]\n",
        "# Calculo da pseudo-inversa\n",
        "pseudo_inversa = np.matmul(np.linalg.inv(np.matmul(A.T, A)), A.T)\n",
        "# Achando os parâmetros\n",
        "ans = np.matmul(pseudo_inversa, b)\n",
        "print(ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmkpXzCsehty",
        "colab_type": "text"
      },
      "source": [
        "Por último podemos comparar o resultado obtido com a função real"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "filNhYv9enny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure(figsize=(16, 8), facecolor='w')\n",
        "# Plota os dados\n",
        "plt.plot(dados[0], dados[1], 'ro', markersize=0.5)\n",
        "# Plota a função usada para gerar dados\n",
        "x = dados[0]\n",
        "y = 3*x**3 + 5*np.exp(-x) + 2\n",
        "plt.plot(x, y, label='Função referência')\n",
        "# Plota a função encontrada\n",
        "plt.plot(dados[0], np.matmul(A, ans), 'g', label='Função encontrada')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El-JrY_BgG5H",
        "colab_type": "text"
      },
      "source": [
        "Como podemos ver praticamente não existe distinção entre as duas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kC1mzBggayu",
        "colab_type": "text"
      },
      "source": [
        "### Outros algorítimos\n",
        "\n",
        "Existem diverços outros algorítimos de otimização que não puderam entrar aqui mas que vale a pena dar uma olhada. Segue a lista de alguns algorítimos muito interessantes:\n",
        "\n",
        " - [Simulated annealing](https://brilliant.org/wiki/annealing/)\n",
        " - [Particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization)\n",
        " - [Ant colony optimization](https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms)\n",
        " - [Programação linear](https://brilliant.org/wiki/linear-programming/)\n",
        " - [Tabu search](http://www.cleveralgorithms.com/nature-inspired/stochastic/tabu_search.html)"
      ]
    }
  ]
}